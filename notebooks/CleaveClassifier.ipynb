{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca20d7b2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomBrightness, RandomZoom, GaussianNoise, RandomContrast\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, concatenate, Input, Dropout, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from keras_tuner import HyperModel, Hyperband\n",
    "from tensorflow.keras.layers import ReLU, AveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762cf82b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DataCollector:\n",
    "  '''\n",
    "  This class collects data from a csv file and image folder saved in google drive and\n",
    "  creates datasets for training a machine learning model.\n",
    "  '''\n",
    "  def __init__(self, csv_path, img_folder):\n",
    "    '''\n",
    "    Parameters:\n",
    "    -----------------------------------------\n",
    "    csv_path: str\n",
    "      - path to csv file in google drive\n",
    "    img_folder: str\n",
    "      - path to image folder in google drive\n",
    "    '''\n",
    "    if csv_path is None or img_folder is None:\n",
    "        raise ValueError(\"Must provide data path\")\n",
    "    self.csv_path = csv_path\n",
    "    self.img_folder = img_folder\n",
    "    self.df = self.clean_data()\n",
    "    self.scaler = None\n",
    "\n",
    "  def clean_data(self):\n",
    "    '''\n",
    "    Read csv file into dataframe and add column for cleave quality.\n",
    "\n",
    "    Returns: pandas.DataFrame\n",
    "      - dataframe with cleave quality column\n",
    "    '''\n",
    "    try:\n",
    "      df = pd.read_csv(self.csv_path)\n",
    "    except FileNotFoundError:\n",
    "      print(\"Csv file not found!\")\n",
    "      return None\n",
    "    df['CleaveQuality'] = ((df['CleaveAngle'] <= 0.45) & (df['Misting'] == 0) & (df['Hackle'] == 0)).astype(int)\n",
    "    # Clean image path to read from google drive'\n",
    "    df['ImagePath'] = df['ImagePath'].str.replace(self.img_folder, \"\")\n",
    "    return df\n",
    "\n",
    "  def load_process_images(self, filename):\n",
    "    \n",
    "    '''\n",
    "    Load image from path in google drive and standardize to 224x224.\n",
    "\n",
    "    Parameters:\n",
    "    -----------------------------------------\n",
    "    filename: str\n",
    "      - path to image in google drive\n",
    "\n",
    "    Returns: tf.tensor\n",
    "      - image in tensor format\n",
    "    '''\n",
    "    def _load_image(file):\n",
    "      file = file.numpy().decode('utf-8')\n",
    "      full_path = os.path.join(self.img_folder, file)\n",
    "      try:\n",
    "        img_raw = tf.io.read_file(full_path)\n",
    "      except FileNotFoundError:\n",
    "        print(\"File not found\")\n",
    "        return None\n",
    "      img = tf.image.decode_png(img_raw, channels=1)\n",
    "      img = tf.image.resize(img, [224, 224])\n",
    "      img = tf.image.grayscale_to_rgb(img)\n",
    "      img = img / 255.0\n",
    "      return img\n",
    "\n",
    "    img = tf.py_function(_load_image, [filename], tf.float32)\n",
    "    img.set_shape([224, 224, 3])\n",
    "    return img\n",
    "\n",
    "  def extract_data(self, feature_scaler_path=None):\n",
    "    '''\n",
    "    Extract data from dataframe into separate lists for creating datasets.\n",
    "\n",
    "    Parameters:\n",
    "    ------------------------------------\n",
    "\n",
    "    scalar_filename: str\n",
    "      - path to store pickled scaler \n",
    "\n",
    "    Returns: list, list, list\n",
    "      - lists of images, features, and labels\n",
    "    '''\n",
    "    images = self.df['ImagePath'].values\n",
    "    #features = self.df[['CleaveAngle', 'CleaveTension']].values\n",
    "    features = self.df[['CleaveAngle', 'CleaveTension', 'ScribeDiameter', 'Misting', 'Hackle', 'Tearing']].values.astype(np.float32)\n",
    "    labels = self.df['CleaveQuality'].values.astype(np.float32)\n",
    "    self.scaler = MinMaxScaler()\n",
    "    features = self.scaler.fit_transform(features)\n",
    "    #joblib.dump(self.scaler, f'./{scaler_filename}.pkl')\n",
    "    if feature_scaler_path:\n",
    "      joblib.dump(self.scaler, f'{feature_scaler_path}.pkl')\n",
    "    return images, features, labels\n",
    "\n",
    "  def process_images_features(self, inputs, label):\n",
    "    # Wrapper function for calling image processing\n",
    "    image_input, features = inputs\n",
    "    image = self.load_process_images(image_input)\n",
    "    return (image, features), label\n",
    "  \n",
    "  def create_kfold_datasets(self, images, features, labels, buffer_size, batch_size, n_splits=5):\n",
    "    '''\n",
    "    Create datasets based on stratified k-fold process for binary classification.\n",
    "\n",
    "    Parameters:\n",
    "    --------------------------------------------------------------------\n",
    "\n",
    "    images: list\n",
    "      - list of image paths\n",
    "    features: list\n",
    "      - list of numerical features\n",
    "    labels: list\n",
    "      - list of target values for classification\n",
    "    buffer_size: int\n",
    "      - size of buffer to perform shuffling\n",
    "    batch_size: int\n",
    "      - size to group data in for training\n",
    "    n_splits\n",
    "      - number of k folds\n",
    "\n",
    "    Returns: list of tuples\n",
    "      - (train_ds, test_ds)\n",
    "    \n",
    "    '''\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=24)\n",
    "\n",
    "    datasets = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X=features, y=labels):\n",
    "      train_imgs, test_imgs = images[train_index], images[test_index]\n",
    "      train_features, test_features = features[train_index], features[test_index]\n",
    "      train_labels, test_labels = labels[train_index], labels[test_index]\n",
    "\n",
    "      train_ds = tf.data.Dataset.from_tensor_slices(((train_imgs, train_features), train_labels))\n",
    "      test_ds = tf.data.Dataset.from_tensor_slices(((test_imgs, test_features), test_labels))\n",
    "\n",
    "      train_ds = train_ds.map(lambda x, y: self.process_images_features(x, y))\n",
    "      test_ds = test_ds.map(lambda x, y: self.process_images_features(x, y))\n",
    "\n",
    "      train_ds = train_ds.shuffle(buffer_size=buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "      test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "      datasets.append((train_ds, test_ds))\n",
    "\n",
    "    return datasets\n",
    "  \n",
    "  def create_datasets(self, images, features, labels, test_size, buffer_size, batch_size):\n",
    "    '''\n",
    "    Creates test and train datasets and splits into different batches after shuffling.\n",
    "\n",
    "    Parameters:\n",
    "    -----------------------------------------\n",
    "\n",
    "    images: list\n",
    "      - paths to images in google drive\n",
    "    features: list\n",
    "      - numerical parameters to label images\n",
    "    labels: int\n",
    "      - targets to qualify image quality\n",
    "    test_size: float\n",
    "      - decimal between 0 and 1 to represent test size of dataset\n",
    "    buffer_size: int\n",
    "      - size of buffer for shuffling data\n",
    "    batch_size: int\n",
    "      - size to group data into\n",
    "\n",
    "    Returns: tf.tensor\n",
    "      - train and test datasets\n",
    "    '''\n",
    "    train_imgs, test_imgs, train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        images, features, labels, stratify=labels, test_size=test_size)\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(((train_imgs, train_features), train_labels))\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices(((test_imgs, test_features), test_labels))\n",
    "\n",
    "    # Map using bound method\n",
    "    train_ds = train_ds.map(lambda x, y: self.process_images_features(x, y))\n",
    "    test_ds = test_ds.map(lambda x, y: self.process_images_features(x, y))\n",
    "\n",
    "    train_ds = train_ds.shuffle(buffer_size=buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6431c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel:\n",
    "    '''\n",
    "    Class is used to define custom model using pre-trained MobileNetV2 model.\n",
    "    '''\n",
    "    def __init__(self, train_ds, test_ds):\n",
    "      self.train_ds = train_ds\n",
    "      self.test_ds = test_ds\n",
    "\n",
    "    def build_pretrained_model(self, image_shape, param_shape):\n",
    "      '''\n",
    "      Utilize pretrained CNN to supplement small dataset\n",
    "\n",
    "      Parameters:\n",
    "      ------------------------------------------------\n",
    "\n",
    "      image_shape: tuple\n",
    "        - dimensions of image\n",
    "      param_shape: tuple\n",
    "        - dimension of features\n",
    "\n",
    "      Returns: tf.keras.Model\n",
    "        - returns model to train\n",
    "      '''\n",
    "      pre_trained_model = MobileNetV2(input_shape=image_shape, include_top=False, weights=\"imagenet\")\n",
    "      pre_trained_model.trainable =False\n",
    "\n",
    "      # Data augmentation pipeline\n",
    "      data_augmentation = Sequential([\n",
    "            RandomFlip(mode=\"HORIZONTAL_AND_VERTICAL\"),\n",
    "            RandomRotation(factor=(0.2)),\n",
    "            RandomBrightness(factor=(0.2)),\n",
    "            RandomZoom(height_factor=0.1, width_factor=0.1),\n",
    "            GaussianNoise(stddev=0.01),\n",
    "            RandomContrast(0.2)\n",
    "        ])\n",
    "      # CNN for images\n",
    "      image_input = Input(shape=image_shape)\n",
    "      x = data_augmentation(image_input, training=True)\n",
    "      x = pre_trained_model(image_input, training=False)\n",
    "      x = GlobalAveragePooling2D()(x)\n",
    "      x = Dropout(0.5)(x)\n",
    "\n",
    "      # Numerical featuers section\n",
    "      params_input = Input(shape=param_shape)\n",
    "      y = Dense(32, activation='relu')(params_input)\n",
    "      y = Dense(16, activation='relu')(y)\n",
    "\n",
    "      combined = concatenate([x, y])\n",
    "      z = Dense(64, activation='relu')(combined)\n",
    "      z = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "      model = Model(inputs=[image_input, params_input], outputs=z)\n",
    "      model.summary()\n",
    "      return model\n",
    "\n",
    "    def compile_model(self, image_shape, param_shape, learning_rate=0.001, metrics=['accuracy', 'precision', 'recall']):\n",
    "      '''\n",
    "      Compile model after calling build_model function\n",
    "\n",
    "      Parameters:\n",
    "      -------------------------------------\n",
    "      image_shape: tuple\n",
    "          - dimensions of images\n",
    "      param_shape: tuple\n",
    "          - dimensions of parameters\n",
    "      learning_rate: float\n",
    "          - learning rate for training model\n",
    "        metrics: list\n",
    "          - metrics to monitor during training\n",
    "          - default: accuracy\n",
    "\n",
    "      Returns:\n",
    "      tf.keras.Model\n",
    "          - Mode to be trained\n",
    "      '''\n",
    "      # Adaptive Moment Estimation optimizer\n",
    "      # Set learning rate and then compile model\n",
    "      # Loss functions is binary_crossentropy for binary classification\n",
    "      #model = build_model((image_shape), (param_shape))\n",
    "      model = self.build_pretrained_model(image_shape, param_shape)\n",
    "      optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "      model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "      return model\n",
    "    \n",
    "    def create_checkpoints(self, checkpoint_filepath=\"/content/drive/MyDrive/checkpoints.keras\", monitor=\"val_accuracy\", mode=\"max\", save_best_only=True):\n",
    "      '''\n",
    "      Create model checkpoints to avoid losing data while training\n",
    "\n",
    "      Parameters:\n",
    "      --------------------------------------\n",
    "\n",
    "      checkpoint_filepath: str\n",
    "        - path to save model checkpoints\n",
    "        - default: /content/drive/MyDrive/checkpoints.keras\n",
    "      monitor: str\n",
    "        - metric to monitor during training\n",
    "        - deafault: val_accuracy\n",
    "      mode: str\n",
    "        - max, min, avg\n",
    "        - method to determine stoppping point of metric\n",
    "        - default: max\n",
    "      save_best_only: boolean\n",
    "        - to determine if only best model shold be saved\n",
    "        - deafault: True\n",
    "\n",
    "      Returns: tf.callback.ModelCheckpoint\n",
    "        - checkpoint to use during training\n",
    "      '''\n",
    "      model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = checkpoint_filepath,\n",
    "        monitor=monitor,\n",
    "        mode=mode,\n",
    "        save_best_only=save_best_only,\n",
    "        verbose=1\n",
    "      )\n",
    "      return model_checkpoint_callback\n",
    "    \n",
    "    def create_early_stopping(self, patience=3, mode='max', monitor=\"val_accuracy\"):\n",
    "      '''\n",
    "      Create early stopping callback to monitor training success and prevent overfitting.\n",
    "\n",
    "      Parameters:\n",
    "      ----------------------------------------\n",
    "\n",
    "      patience: int\n",
    "        - number of epochs to stop when monitor plateus\n",
    "        - default: 3\n",
    "      mode: str\n",
    "        - max, min, avg\n",
    "        - method to track monitor\n",
    "        - default: max\n",
    "      monitor: str\n",
    "        - metric to monitor during training\n",
    "        - default: val_accuracy\n",
    "      \n",
    "      Returns: tf.callbacks.EarlyStopping\n",
    "        - early stopping callback\n",
    "      '''\n",
    "      es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=monitor,\n",
    "        patience=patience,\n",
    "        mode = mode,\n",
    "        restore_best_weights=True\n",
    "      )\n",
    "      return es_callback\n",
    "    \n",
    "    def train_model(self, model, checkpoints=None, epochs=5, initial_epoch=0, early_stopping=None, history_file=None, model_file=None):\n",
    "      '''\n",
    "      Train model with possible callbacks to prevent overfitting\n",
    "\n",
    "      Parameters:\n",
    "      -----------------------------------------\n",
    "\n",
    "      model: tf.keras.Model\n",
    "        - model to be trained\n",
    "      checkpoints: tf.keras.callback.Checkpoints\n",
    "        - checkpoints to save model\n",
    "        - default: None\n",
    "      epochs: int\n",
    "        - number of training epochs to pass though\n",
    "        - default: 5\n",
    "      early_stopping: tf.keras.callback.EarlyStopping\n",
    "        - early stopping callback to prevent overfitting\n",
    "        - defatult: None\n",
    "      history_file: str\n",
    "        - file to save history to\n",
    "      model_file: str\n",
    "        - file to save model to\n",
    "\n",
    "      Returns: tf.keras.Model\n",
    "        - trained model\n",
    "      '''\n",
    "      callbacks = []\n",
    "      if early_stopping:\n",
    "        callbacks.append(early_stopping)\n",
    "      if checkpoints:\n",
    "        callbacks.append(checkpoints)\n",
    "\n",
    "      if callbacks:\n",
    "        history = model.fit(self.train_ds, epochs=epochs, initial_epoch=initial_epoch,\n",
    "                    validation_data=(self.test_ds), callbacks=callbacks)\n",
    "      else:\n",
    "        print(\"Training without callbacks\")\n",
    "        history = model.fit(self.train_ds, epochs=epochs, initial_epoch=initial_epoch,\n",
    "                    validation_data=(self.test_ds))\n",
    "      if history_file:\n",
    "        df = pd.DataFrame(history.history)\n",
    "        df.to_csv(f\"{history_file}.csv\", index=False)\n",
    "      else:\n",
    "        print(\"History not saved\")\n",
    "      if model_file:\n",
    "        model.save(f'{model_file}.keras')\n",
    "      else:\n",
    "        print(\"Model not saved\")\n",
    "      return history\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_kfold( datasets, image_shape, param_shape, learning_rate, metrics = ['accuracy', 'precision', 'recall'], checkpoints=None, epochs=5, initial_epoch=0, early_stopping=None, history_file=None, model_file=None):\n",
    "      kfold_histories = []\n",
    "      k_models = []\n",
    "      train_datasets = [i[0] for i in datasets]\n",
    "      test_datasets = [i[1] for i in datasets]\n",
    "\n",
    "      callbacks=[]\n",
    "\n",
    "      if early_stopping:\n",
    "        callbacks.append(early_stopping)\n",
    "      if checkpoints:\n",
    "        callbacks.append(checkpoints)\n",
    "\n",
    "      for fold, (train_ds, test_ds) in enumerate(zip(train_datasets, test_datasets)):\n",
    "        print(f\"\\n=== Training fold {fold + 1} ===\")\n",
    "\n",
    "        custom_model = CustomModel(train_ds, test_ds)\n",
    "        model = custom_model.compile_model(image_shape=image_shape, param_shape=param_shape, learning_rate=learning_rate, metrics=metrics)\n",
    "\n",
    "        if callbacks:\n",
    "          history = model.fit(train_ds, epochs=epochs, initial_epoch=initial_epoch,\n",
    "                    validation_data=(test_ds), callbacks=callbacks)\n",
    "        else:\n",
    "          print(\"Training without callbacks\")\n",
    "          history = model.fit(train_ds, epochs=epochs, initial_epoch=initial_epoch,\n",
    "                    validation_data=(test_ds))\n",
    "          \n",
    "        kfold_histories.append(history)\n",
    "        k_models.append(model)\n",
    "          \n",
    "        if history_file:\n",
    "          df = pd.DataFrame(history.history)\n",
    "          df.to_csv(f\"{history_file}_fold{fold+1}.csv\", index=False)\n",
    "        else:\n",
    "          print(\"History not saved\")\n",
    "        if model_file:\n",
    "          model.save(f'{model_file}_fold{fold+1}.keras')\n",
    "        else:\n",
    "         print(\"Model not saved\")\n",
    "\n",
    "      return k_models, kfold_histories\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_averages_from_kfold(kfold_histories):\n",
    "      accuracy = []\n",
    "      precision = []\n",
    "      recall = []\n",
    "\n",
    "      for history in kfold_histories:\n",
    "        accuracy.append(max(history['accuracy']))\n",
    "        precision.append(max(history['precision']))\n",
    "        recall.append(max(history['recall']))\n",
    "\n",
    "      avg_accuracy = np.mean(accuracy)\n",
    "      avg_precision = np.mean(precision)\n",
    "      avg_recall = np.mean(recall)\n",
    "\n",
    "      print(f\"Average Accuracy: {avg_accuracy:.2f}\")\n",
    "      print(f\"Average Precision: {avg_precision:.2f}\")\n",
    "      print(f\"Average Recall: {avg_recall:.2f}\")\n",
    "\n",
    "\n",
    "    def plot_metric(self, title, metric_1, metric_2, metric_1_label, metric_2_label, x_label, y_label):\n",
    "      '''\n",
    "      Plotting function for one metric\n",
    "\n",
    "      Parameters:\n",
    "      ----------------------------------------------\n",
    "\n",
    "      title: str\n",
    "        - title for plot\n",
    "      metric_1, metric_2: strs\n",
    "        - metrics to be plotted vs. each other\n",
    "      metric_1_label, metric_2_label: strs\n",
    "        - labels for each metric to plot\n",
    "      x_label, y_label: strs\n",
    "        - labels for graph axes\n",
    "      '''\n",
    "\n",
    "      plt.title(title)\n",
    "      plt.plot(metric_1, label=metric_1_label)\n",
    "      plt.plot(metric_2, label=metric_2_label)\n",
    "      plt.xlabel(x_label)\n",
    "      plt.ylabel(y_label)\n",
    "      plt.legend(loc=\"lower right\")\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fecf546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c55d9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TestPredictions:\n",
    "  '''\n",
    "  This class is used to test model performance on unseen data using metrics such as\n",
    "  accuracy, precision, recall, and confusion matrix.\n",
    "  '''\n",
    "  def __init__(self, model_path, csv_path, scalar_path, img_folder):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ----------------------------------------------\n",
    "\n",
    "    model_path: str\n",
    "      - path to model in google drive\n",
    "    csv_path: str\n",
    "      - path to csv file in google drive\n",
    "    scalar_path: str\n",
    "      - path to scaler in google drive\n",
    "    '''\n",
    "    self.scalar_path = scalar_path\n",
    "    self.img_folder = img_folder\n",
    "    self.model = tf.keras.models.load_model(model_path)\n",
    "    self.csv_path = csv_path\n",
    "\n",
    "  def clean_data(self):\n",
    "    '''\n",
    "    Read csv file into dataframe and add column for cleave quality.\n",
    "\n",
    "    Returns: pandas.DataFrame\n",
    "      - dataframe with cleave quality column\n",
    "    '''\n",
    "    try:\n",
    "      df = pd.read_csv(self.csv_path)\n",
    "    except FileNotFoundError:\n",
    "      print(\"File not found\")\n",
    "      return None\n",
    "    df['CleaveQuality'] = ((df['CleaveAngle'] <= 0.45) & (df['Misting'] == 0) & (df['Hackle'] == 0)).astype(int)\n",
    "    # Clean image path to read from google drive\n",
    "    df['ImagePath'] = df['ImagePath'].str.replace(\"C:\\\\Thorlabs\\\\125PM\\\\\", \"\")    \n",
    "    pred_image_paths = df['ImagePath'].values\n",
    "    pred_features = df[['CleaveAngle', 'CleaveTension', 'ScribeDiameter', 'Misting', 'Hackle', 'Tearing']].values.astype(np.float32)\n",
    "    self.true_labels = list(df['CleaveQuality'])\n",
    "    return pred_image_paths, pred_features\n",
    "\n",
    "  def load_process_images(self, filename):\n",
    "    '''\n",
    "    Load image from path in google drive and standardize to 224x224.\n",
    "\n",
    "    Parameters:\n",
    "    -----------------------------------------\n",
    "    filename: str\n",
    "      - path to image in google drive\n",
    "\n",
    "    Returns: tf.tensor\n",
    "      - image in tensor format\n",
    "    '''\n",
    "    def _load_image(file):\n",
    "      file = file.numpy().decode('utf-8')\n",
    "      full_path = os.path.join(self.img_folder, file)\n",
    "      try:\n",
    "        img_raw = tf.io.read_file(full_path)\n",
    "      except FileNotFoundError:\n",
    "        print(\"File not found\")\n",
    "        return None\n",
    "      img = tf.image.decode_png(img_raw, channels=1)\n",
    "      img = tf.image.resize(img, [224, 224])\n",
    "      img = tf.image.grayscale_to_rgb(img)\n",
    "      img = img / 255.0\n",
    "      return img\n",
    "\n",
    "    img = tf.py_function(_load_image, [filename], tf.float32)\n",
    "    img.set_shape([224, 224, 3])\n",
    "    return img\n",
    "\n",
    "  def test_prediction(self, image_path, feature_vector):\n",
    "    '''\n",
    "    Test function for generating prediction\n",
    "\n",
    "    Parameters:\n",
    "    ----------------------------------------------\n",
    "\n",
    "    image_path: str\n",
    "      - path to image to predict\n",
    "    tension: int\n",
    "      - tension value in grams\n",
    "    cleave_angle: float\n",
    "      - angle that was achieved from cleave\n",
    "\n",
    "    Return: tf.keras.Model\n",
    "      - predicition from new image of good or bad cleave\n",
    "    '''\n",
    "    image = self.load_process_images(image_path)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    scalar = joblib.load(self.scalar_path)\n",
    "    scaled_features = scalar.transform([feature_vector]) \n",
    "\n",
    "    prediction = self.model.predict([image, scaled_features])\n",
    "    return prediction\n",
    "\n",
    "  def gather_predictions(self):\n",
    "    '''\n",
    "    Gather multiple predictions from test data\n",
    "\n",
    "    Returns: list\n",
    "      - list of predictions\n",
    "    '''\n",
    "\n",
    "    pred_image_paths, pred_features = self.clean_data()\n",
    "    predictions = []\n",
    "    for img_path, feature_vector in zip(pred_image_paths, pred_features):\n",
    "      prediction = self.test_prediction(img_path, feature_vector)\n",
    "      predictions.append(prediction)\n",
    "\n",
    "    # Set prediction labels to 0 or 1 based on probability\n",
    "    pred_labels = [1 if pred[0][0] > 0.5 else 0 for pred in predictions]\n",
    "    return pred_labels, predictions\n",
    "\n",
    "  def display_confusion_matrix(self, pred_labels):\n",
    "    '''\n",
    "    Displays confusion matri metric comparing true labels to predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    ----------------------------------------------\n",
    "\n",
    "    pred_labels: list\n",
    "      - list of predicted labels\n",
    "    '''\n",
    "    cm = confusion_matrix(self.true_labels, pred_labels, labels=[0, 1])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=['Bad Cleave', 'Good Cleave'])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "  def display_classification_report(self, true_labels, pred_labels):\n",
    "    '''\n",
    "    Diplays classification report comparing true labels to predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    ----------------------------------------------\n",
    "\n",
    "    true_labels: list\n",
    "      - list of true labels\n",
    "    pred_labels: list\n",
    "      - list of predicted labels\n",
    "    '''\n",
    "    print(classification_report(true_labels, pred_labels))\n",
    "\n",
    "  def plot_roc(self, title, true_labels, pred_probabilites):\n",
    "    pred_probabilites = np.array(pred_probabilites).flatten()\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels, pred_probabilites)\n",
    "\n",
    "    auc = roc_auc_score(true_labels, pred_probabilites)\n",
    "\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC={auc:.2f}%)')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310f0df",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BuildHyperModel(HyperModel):\n",
    "    '''\n",
    "    This class build a HyperModel to determine optimal hyperparmeters\n",
    "    '''\n",
    "    def __init__(self, image_shape, param_shape):\n",
    "      '''\n",
    "      Parameters:\n",
    "      ----------------------------------------------\n",
    "\n",
    "      image_shape: tuple\n",
    "        - dimensions of image\n",
    "      param_shape: tuple\n",
    "        - dimensions of parameters\n",
    "      '''\n",
    "      self.image_shape = image_shape\n",
    "      self.param_shape = param_shape\n",
    "\n",
    "    def build(self, hp):\n",
    "      '''\n",
    "      Build hypermodel to perform hyperparameter search.\n",
    "\n",
    "      Parameters:\n",
    "      -------------------------\n",
    "\n",
    "      hp: keras_tuner.engine.hyperparameters.HyperParameters\n",
    "        - hyperparameters to be tuned\n",
    "      '''\n",
    "        # Pre-trained base model\n",
    "      pre_trained_model = MobileNetV2(\n",
    "            input_shape=self.image_shape,\n",
    "            include_top=False,\n",
    "            weights=\"imagenet\"\n",
    "        )\n",
    "      pre_trained_model.trainable = False\n",
    "\n",
    "        # Data augmentation pipeline\n",
    "      data_augmentation = Sequential([\n",
    "            RandomFlip(mode=\"HORIZONTAL_AND_VERTICAL\"),\n",
    "            RandomRotation(factor=(0.2)),\n",
    "            RandomBrightness(factor=(0.2)),\n",
    "            RandomZoom(height_factor=0.1, width_factor=0.1),\n",
    "            GaussianNoise(stddev=0.01),\n",
    "            RandomContrast(0.2)\n",
    "        ])\n",
    "\n",
    "        # Image input and processing\n",
    "      image_input = Input(shape=self.image_shape)\n",
    "      x = data_augmentation(image_input)\n",
    "      x = pre_trained_model(x, training=False)\n",
    "      x = GlobalAveragePooling2D()(x)\n",
    "      x = Dropout(hp.Float('dropout', 0.2, 0.5, step=0.1))(x)\n",
    "\n",
    "        # Param input and processing\n",
    "      param_input = Input(shape=self.param_shape)\n",
    "      y = Dense(\n",
    "            hp.Int('dense_param1', min_value=16, max_value=128, step=16),\n",
    "            activation='relu')(param_input)\n",
    "      y = Dense(\n",
    "            hp.Int('dense_param2', min_value=8, max_value=64, step=8),\n",
    "            activation='relu')(y)\n",
    "\n",
    "        # Combine image and parameter features\n",
    "      combined = concatenate([x, y])\n",
    "\n",
    "      z = Dense(\n",
    "            hp.Int('dense_combined', min_value=16, max_value=128, step=16),\n",
    "            activation='relu')(combined)\n",
    "      z = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "      model = Model(inputs=[image_input, param_input], outputs=z)\n",
    "\n",
    "      model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=hp.Choice('learning_rate', values=[0.0005, 0.001, 0.01])\n",
    "            ),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', 'precision','recall']\n",
    "        )\n",
    "\n",
    "      return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameterTuning:\n",
    "  '''\n",
    "  This class is used to tune hyperparameters for model\n",
    "  '''\n",
    "  def __init__(self, image_shape, feature_shape, max_epochs=20, objective='val_accuracy', directory='/content/drive/MyDrive/Thorlabs', project_name='Cleave_Tuner3'):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ----------------------------------------------\n",
    "\n",
    "    image_shape: tuple\n",
    "      - dimensions of image\n",
    "    feature_shape: tuple\n",
    "      - dimensions of parameters\n",
    "    max_epochs: int\n",
    "      - maximum number of epochs to train for\n",
    "      - default: 20\n",
    "    objective: str\n",
    "      - metric to monitor during tuning\n",
    "      - default: val_accuracy\n",
    "    directory: str\n",
    "      - directory path to store hyperparameters\n",
    "      - deafult: /content/drive/MyDrive/Thorlabs\n",
    "    project_name: str\n",
    "      - name of project\n",
    "      - deafult: Cleave_Tuner3\n",
    "\n",
    "    '''\n",
    "    self.image_shape = image_shape\n",
    "    self.feature_shape = feature_shape\n",
    "    hypermodel = BuildHyperModel(self.image_shape, self.feature_shape)\n",
    "    self.tuner = Hyperband(\n",
    "        hypermodel,\n",
    "        objective=objective,\n",
    "        max_epochs=max_epochs,\n",
    "        directory=directory,\n",
    "        project_name=project_name\n",
    "    )\n",
    "  def run_search(self, train_ds, test_ds):\n",
    "    '''\n",
    "    Run hyperparameter search\n",
    "\n",
    "    Parameters:\n",
    "    ----------------------------------------------\n",
    "\n",
    "    train_ds: tf.data.Dataset\n",
    "      - training dataset\n",
    "    test_ds: tf.data.Dataset\n",
    "      - testing dataset\n",
    "      \n",
    "    '''\n",
    "  \n",
    "    self.tuner.search(train_ds, validation_data=test_ds)\n",
    "\n",
    "  def get_best_model(self):\n",
    "    '''\n",
    "    Get best model from hyperparameter search\n",
    "\n",
    "    Returns: tf.keras.Model\n",
    "      - best model from hyperparameter search\n",
    "    '''\n",
    "    return self.tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "  def get_best_hyperparameters(self):\n",
    "    '''\n",
    "    Get best hyperparameters from hyperparameter search\n",
    "\n",
    "    Returns: keras_tuner.engine.hyperparameters.HyperParameters\n",
    "      - best hyperparameters from hyperparameter search\n",
    "    '''\n",
    "    return self.tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
