{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d9cc3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import joblib\n",
    "from keras_tuner.tuners import Hyperband\n",
    "import matplotlib.pyplot as plt\n",
    "from keras_tuner import HyperModel\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from CleaveClassifier import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be3caf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MLPDataCollector(DataCollector):\n",
    "\n",
    "    def __init__(self, csv_path, img_folder):\n",
    "        super().__init__(csv_path, img_folder)\n",
    "        \n",
    "\n",
    "    def extract_data(self, feature_scaler_path=None, tension_scaler_path=None):\n",
    "        '''\n",
    "        Extract data from dataframe into separate lists for creating datasets.\n",
    "\n",
    "        Parameters:\n",
    "        ------------------------------------\n",
    "\n",
    "        scalar_filename: str\n",
    "        - path to store pickled scaler \n",
    "\n",
    "        Returns: list, list, list\n",
    "        - lists of images, features, and labels\n",
    "        '''\n",
    "        images = self.df['ImagePath'].values\n",
    "        #features = self.df[['CleaveAngle', 'CleaveTension']].values\n",
    "        features = self.df[['CleaveAngle', 'ScribeDiameter', 'Misting', 'Hackle', 'Tearing']].values.astype(np.float32)\n",
    "        labels = self.df['CleaveTension'].values.astype(np.float32)\n",
    "        tension_scaler = MinMaxScaler()\n",
    "        labels = tension_scaler.fit_transform(labels.reshape(-1, 1))\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        features = feature_scaler.fit_transform(features)\n",
    "        if feature_scaler_path:\n",
    "            joblib.dump(self.scaler, f'{feature_scaler_path}.pkl')\n",
    "        if tension_scaler_path:\n",
    "            joblib.dump(self.scaler, f'{tension_scaler_path}.pkl')\n",
    "        return images, features, labels\n",
    "    \n",
    "    def create_datasets(self, images, features, labels, test_size, buffer_size, batch_size):\n",
    "        '''\n",
    "        Creates test and train datasets and splits into different batches after shuffling.\n",
    "\n",
    "        Parameters:\n",
    "        -----------------------------------------\n",
    "\n",
    "        images: list\n",
    "        - paths to images in google drive\n",
    "        features: list\n",
    "        - numerical parameters to label images\n",
    "        labels: int\n",
    "        - targets to qualify image quality\n",
    "        test_size: float\n",
    "        - decimal between 0 and 1 to represent test size of dataset\n",
    "        buffer_size: int\n",
    "        - size of buffer for shuffling data\n",
    "        batch_size: int\n",
    "        - size to group data into\n",
    "\n",
    "        Returns: tf.tensor\n",
    "        - train and test datasets\n",
    "        '''\n",
    "        train_imgs, test_imgs, train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "            images, features, labels, test_size=test_size)\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices(((train_imgs, train_features), train_labels))\n",
    "        test_ds = tf.data.Dataset.from_tensor_slices(((test_imgs, test_features), test_labels))\n",
    "\n",
    "        # Map using bound method\n",
    "        train_ds = train_ds.map(lambda x, y: self.process_images_features(x, y))\n",
    "        test_ds = test_ds.map(lambda x, y: self.process_images_features(x, y))\n",
    "\n",
    "        train_ds = train_ds.shuffle(buffer_size=buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d24f4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BuildMLPModel(CustomModel):\n",
    "\n",
    "    def __init__(self, cnn_model_path, train_ds, test_ds):\n",
    "        super().__init__(train_ds, test_ds)\n",
    "        self.cnn_model = tf.keras.models.load_model(cnn_model_path)\n",
    "        self.image_input = self.cnn_model.input[0]\n",
    "        self.feature_output = self.cnn_model.get_layer('dropout').output\n",
    "       \n",
    "\n",
    "    def build_pretrained_model(self, param_shape):\n",
    "        '''\n",
    "        Build model\n",
    "\n",
    "        Returns:\n",
    "        tf.keras.Model\n",
    "            - Model to be trained\n",
    "            \n",
    "        '''\n",
    "        # Pre-trained base model\n",
    "        x = Dense(64, activation='relu')(self.feature_output)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        feature_input = Input(shape=param_shape, name='feature_input')  # Features\n",
    "        #angle_input = Input(shape=(1,), name='angle_input')  # New input\n",
    "        y = Dense(16, activation='relu')(feature_input)\n",
    "        #y = Dense(16, activation='relu')(angle_input\n",
    "\n",
    "        combined = Concatenate()([x, y])\n",
    "        z = Dense(64, activation='relu')(combined)\n",
    "        output = Dense(1, name='tension_output')(z)\n",
    "        # Use angle input for 250LA\n",
    "        regression_model = Model(inputs=[self.image_input, feature_input], outputs=output)\n",
    "        regression_model.summary()\n",
    "        return regression_model\n",
    "    \n",
    "    def compile_model(self, param_shape, learning_rate=0.001):\n",
    "      '''\n",
    "      Compile model after calling build_model function\n",
    "\n",
    "      Parameters:\n",
    "      -------------------------------------\n",
    "      image_shape: tuple\n",
    "          - dimensions of images\n",
    "      param_shape: tuple\n",
    "          - dimensions of parameters\n",
    "      learning_rate: float\n",
    "          - learning rate for training model\n",
    "\n",
    "      Returns:\n",
    "      tf.keras.Model\n",
    "          - Mode to be trained\n",
    "      '''\n",
    "      # Adaptive Moment Estimation optimizer\n",
    "      # Set learning rate and then compile model\n",
    "      # Loss functions is mean squared error for regression\n",
    "      model = self.build_pretrained_model(param_shape)\n",
    "      optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "      model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "      return model\n",
    "    \n",
    "    def create_early_stopping(self, patience=3, mode='min', monitor=\"val_mae\"):\n",
    "      '''\n",
    "      Create early stopping callback to monitor training success and prevent overfitting.\n",
    "\n",
    "      Parameters:\n",
    "      ----------------------------------------\n",
    "\n",
    "      patience: int\n",
    "        - number of epochs to stop when monitor plateus\n",
    "        - default: 3\n",
    "      mode: str\n",
    "        - max, min, avg\n",
    "        - method to track monitor\n",
    "        - default: max\n",
    "      monitor: str\n",
    "        - metric to monitor during training\n",
    "        - default: val_accuracy\n",
    "      \n",
    "      Returns: tf.callbacks.EarlyStopping\n",
    "        - early stopping callback\n",
    "      '''\n",
    "      es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=monitor,\n",
    "        patience=patience,\n",
    "        mode = mode,\n",
    "        restore_best_weights=True\n",
    "      )\n",
    "      return es_callback\n",
    "    \n",
    "    def create_checkpoints(self, checkpoint_filepath=\"/content/drive/MyDrive/mlp_checkpoints.keras\", monitor=\"val_mae\", mode=\"min\", save_best_only=True):\n",
    "      '''\n",
    "      Create model checkpoints to avoid losing data while training\n",
    "\n",
    "      Parameters:\n",
    "      --------------------------------------\n",
    "\n",
    "      checkpoint_filepath: str\n",
    "        - path to save model checkpoints\n",
    "        - default: /content/drive/MyDrive/checkpoints.keras\n",
    "      monitor: str\n",
    "        - metric to monitor during training\n",
    "        - deafault: val_accuracy\n",
    "      mode: str\n",
    "        - max, min, avg\n",
    "        - method to determine stoppping point of metric\n",
    "        - default: max\n",
    "      save_best_only: boolean\n",
    "        - to determine if only best model shold be saved\n",
    "        - deafault: True\n",
    "\n",
    "      Returns: tf.callback.ModelCheckpoint\n",
    "        - checkpoint to use during training\n",
    "      '''\n",
    "      model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = checkpoint_filepath,\n",
    "        monitor=monitor,\n",
    "        mode=mode,\n",
    "        save_best_only=save_best_only,\n",
    "        verbose=1\n",
    "      )\n",
    "      return model_checkpoint_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b81b21",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TensionPredictor:\n",
    "\n",
    "    def __init__(self, model, image_folder, image_path, tension_scaler_path, feature_scaler_path):\n",
    "        self.model = model\n",
    "        self.image_path = image_path\n",
    "        self.image_folder = image_folder\n",
    "        self.tenion_scaler = joblib.load(tension_scaler_path)\n",
    "        self.feature_scaler = joblib.load(feature_scaler_path)\n",
    "\n",
    "    def load_and_preprocess_image(self, file_path, img_folder):\n",
    "        '''\n",
    "        Load and preprocess image from file path\n",
    "\n",
    "        Parameters:\n",
    "        -------------------------------------\n",
    "        file_path: str\n",
    "            - path to image file\n",
    "        img_folder: str\n",
    "            - path to image folder\n",
    "            Returns:\n",
    "        tf.Tensor\n",
    "            - preprocessed image\n",
    "        '''\n",
    "        # Construct full path\n",
    "        full_path = os.path.join(img_folder, file_path)\n",
    "        img_raw = tf.io.read_file(full_path)\n",
    "        img = tf.image.decode_png(img_raw, channels=1)\n",
    "        img = tf.image.resize(img, [224, 224])\n",
    "        img = tf.image.grayscale_to_rgb(img)\n",
    "        # Normalize image\n",
    "        img = img / 255.0\n",
    "        return img\n",
    "\n",
    "    def PredictTension(self, features):\n",
    "        '''\n",
    "        Predict tension for given image and angle\n",
    "\n",
    "        Parameters:\n",
    "        -------------------------------------\n",
    "        model: tf.keras.Model\n",
    "            - Model to be used for prediction\n",
    "        image_path: str\n",
    "            - Path to image to be used for prediction\n",
    "            angle: float\n",
    "            - Angle to be used for prediction\n",
    "\n",
    "        Returns:\n",
    "        float\n",
    "            - Predicted tension\n",
    "        '''\n",
    "        # Process image and convert angle and image to tensor with dimension for single batch\n",
    "        image = self.load_and_preprocess_image(self.image_folder, self.image_path)\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "        features = tf.convert_to_tensor(features.values, dtype=tf.float32)\n",
    "        # Predict tension\n",
    "        features = self.feature_scaler.transform(features)\n",
    "        predicted_tension = self.model.predict([image, features])\n",
    "        # Scale tension back to normal units\n",
    "        predicted_tension = self.tension_scaler.inverse_transform(predicted_tension)\n",
    "        # Print tensions\n",
    "        return predicted_tension[0][0]\n",
    "\n",
    "    def plot_metric(self, title, X, y, x_label, y_label, x_legend, y_legend):\n",
    "        '''\n",
    "        Plot metric\n",
    "\n",
    "        Parameters:\n",
    "        -------------------------------------\n",
    "        title: str\n",
    "            - Title of plot\n",
    "        X: list\n",
    "            - List of x values\n",
    "        y: list\n",
    "            - List of y values\n",
    "            x_label: str\n",
    "            - Label for x axis\n",
    "        y_label: str\n",
    "            - Label for y axis\n",
    "        x_legend: str\n",
    "            - Legend for x axis\n",
    "        y_legend: str\n",
    "            - Legend for y axis\n",
    "        '''\n",
    "        plt.title(title)\n",
    "        plt.plot(X, label=x_legend)\n",
    "        plt.plot(y, label=y_legend)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01407b4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BuildMLPHyperModel(HyperModel):\n",
    "    '''\n",
    "    This class build a HyperModel to determine optimal hyperparmeters\n",
    "    '''\n",
    "    def __init__(self, model_path):\n",
    "        '''\n",
    "        Parameters:\n",
    "        -------------------------------------\n",
    "        model: tf.keras.Model\n",
    "            - Model to be used for hyperparameter tuning\n",
    "        '''\n",
    "        self.cnn_model = tf.keras.models.load_model(model_path)\n",
    "        self.image_input = self.cnn_model.input[0]\n",
    "        self.feature_output = self.cnn_model.get_layer('dropout').output\n",
    "\n",
    "    def build(self, hp):\n",
    "      '''\n",
    "      Build model with hyperparameters\n",
    "\n",
    "      Parameters:\n",
    "      -------------------------------------\n",
    "      hp: keras_tuner.HyperParameters\n",
    "          - Hyperparameters to be used for tuning\n",
    "      Returns:\n",
    "      tf.keras.Model\n",
    "          - Model to be trained\n",
    "      '''\n",
    "        # Pre-trained base model\n",
    "\n",
    "      x = Dense(\n",
    "            hp.Int('dense_param1', min_value=16, max_value=128, step=16),\n",
    "            activation='relu')(self.feature_output)\n",
    "      x = Dense(\n",
    "            hp.Int('dense_param2', min_value=8, max_value=64, step=8),\n",
    "            activation='relu')(x)\n",
    "\n",
    "      feature_input = Input(shape=(5,), name='feature_input')  # Features\n",
    "        #angle_input = Input(shape=(1,), name='angle_input')  # New input\n",
    "      y = Dense(\n",
    "            hp.Int('dense_angle', min_value=16, max_value=128, step=16),\n",
    "            activation='relu')(feature_input)\n",
    "\n",
    "      combined = Concatenate()([x, y])\n",
    "      z = Dense(\n",
    "            hp.Int('dense_combined', min_value=16, max_value=128, step=16),\n",
    "            activation='relu')(combined)\n",
    "      z = Dense(1)(z)\n",
    "\n",
    "      mlp_hypermodel = Model(inputs=[self.image_input, feature_input], outputs=z)\n",
    "      mlp_hypermodel.summary()\n",
    "\n",
    "      mlp_hypermodel.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=hp.Choice('learning_rate', values=[0.0005, 0.001, 0.01])\n",
    "            ),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "\n",
    "      return mlp_hypermodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95fb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHyperparameterTuning(HyperParameterTuning):\n",
    "\n",
    "    def __init__(self, cnn_path, max_epochs=20, objective='val_mae', directory='/content/drive/MyDrive/Thorlabs', project_name='MLPTuner'):\n",
    "      '''\n",
    "      Parameters:\n",
    "      -------------------------------------\n",
    "      model: tf.keras.Model\n",
    "          - Model to be used for hyperparameter tuning\n",
    "      '''\n",
    "      self.cnn_model = tf.keras.models.load_model(cnn_path)\n",
    "      self.image_input = self.cnn_model.input[0]\n",
    "      self.feature_output = self.cnn_model.get_layer('dropout').output\n",
    "      hypermodel = BuildMLPHyperModel(cnn_path)\n",
    "      self.tuner = Hyperband(\n",
    "        hypermodel,\n",
    "        objective=objective,\n",
    "        max_epochs=max_epochs,\n",
    "        directory=directory,\n",
    "        project_name=project_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44e42ba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
